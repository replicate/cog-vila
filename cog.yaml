build:
  gpu: true
  cuda: "11.8"
  python_version: "3.10.13"
  python_packages:
    - "https://github.com/Dao-AILab/flash-attention/releases/download/v2.4.2/flash_attn-2.4.2+cu118torch2.0cxx11abiFALSE-cp310-cp310-linux_x86_64.whl"
    - "git+https://github.com/Efficient-Large-Model/VILA.git@1c09a9310f19762d8498ef6dc1aaf45189ceba84"
  run:
    - "pip install git+https://github.com/huggingface/transformers@v4.36.2"
    - "pip install protobuf"
    - "git clone https://github.com/Efficient-Large-Model/VILA.git \
      && cd VILA \
      && git checkout 1c09a9310f19762d8498ef6dc1aaf45189ceba84 \
      && cp -r ./llava/train/transformers_replace/* /root/.pyenv/versions/3.10.13/lib/python3.10/site-packages/transformers/models/"
    - curl -o /usr/local/bin/pget -L https://github.com/replicate/pget/releases/download/v0.6.0/pget_Linux_x86_64
    - chmod +x /usr/local/bin/pget
    # # DONT DELETE-------------------------------------------------------------
    # - "export TORCH_CUDA_ARCH_LIST='8.6'
    #   && export TCNN_CUDA_ARCHITECTURES='86'
    #   && export CUDA_HOME=/usr/local/cuda
    #   && export PATH=${CUDA_HOME}/bin:/home/${USER_NAME}/.local/bin:${PATH}
    #   && export LD_LIBRARY_PATH=${CUDA_HOME}/lib64:${LD_LIBRARY_PATH} 
    #   && export LIBRARY_PATH=${CUDA_HOME}/lib64/stubs:${LIBRARY_PATH}
    #   && git clone https://github.com/mit-han-lab/llm-awq.git \
    #   && cd llm-awq \
    #   && pip install . \
    #   && cd awq/kernels \
    #   && python setup.py install"
    # # - "pip install -U git+https://github.com/huggingface/transformers@v4.36.2"
    # # - "---------------------------------------------------------------------"
    # - "pip install -U transformers==4.32.0"
predict: "predict.py:Predictor"